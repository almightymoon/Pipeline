# ===========================================================
# NVIDIA GPU Operator Configuration
# Enables GPU support and MIG partitioning in Kubernetes
# ===========================================================
apiVersion: v1
kind: Namespace
metadata:
  name: gpu-operator
  labels:
    app.kubernetes.io/name: gpu-operator
    app.kubernetes.io/component: gpu-management
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: gpu-operator
  namespace: gpu-operator
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: gpu-operator
rules:
- apiGroups: [""]
  resources: ["nodes", "pods", "configmaps", "secrets", "serviceaccounts"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["apps"]
  resources: ["daemonsets", "deployments", "replicasets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["rbac.authorization.k8s.io"]
  resources: ["clusterroles", "clusterrolebindings", "roles", "rolebindings"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["apiextensions.k8s.io"]
  resources: ["customresourcedefinitions"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["nvidia.com"]
  resources: ["*"]
  verbs: ["*"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: gpu-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: gpu-operator
subjects:
- kind: ServiceAccount
  name: gpu-operator
  namespace: gpu-operator
---
# ===========================================================
# GPU Node Feature Discovery
# ===========================================================
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nvidia-gpu-feature-discovery
  namespace: gpu-operator
  labels:
    app: nvidia-gpu-feature-discovery
spec:
  selector:
    matchLabels:
      app: nvidia-gpu-feature-discovery
  template:
    metadata:
      labels:
        app: nvidia-gpu-feature-discovery
    spec:
      serviceAccountName: gpu-operator
      hostNetwork: true
      hostPID: true
      containers:
      - name: gpu-feature-discovery
        image: nvcr.io/nvidia/gpu-feature-discovery:v0.8.2
        command:
        - /usr/bin/gpu-feature-discovery
        - -oneshot
        - -output-file=/etc/kubernetes/node-feature-discovery/features.d/nfd-gpu
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        volumeMounts:
        - name: host-root
          mountPath: /host
          readOnly: true
        - name: nfd-features
          mountPath: /etc/kubernetes/node-feature-discovery/features.d
        securityContext:
          privileged: true
      volumes:
      - name: host-root
        hostPath:
          path: /
      - name: nfd-features
        hostPath:
          path: /etc/kubernetes/node-feature-discovery/features.d
      nodeSelector:
        nvidia.com/gpu.present: "true"
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
---
# ===========================================================
# GPU Device Plugin
# ===========================================================
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nvidia-device-plugin-daemonset
  namespace: gpu-operator
  labels:
    app: nvidia-device-plugin
spec:
  selector:
    matchLabels:
      app: nvidia-device-plugin
  template:
    metadata:
      labels:
        app: nvidia-device-plugin
    spec:
      serviceAccountName: gpu-operator
      hostNetwork: true
      hostPID: true
      containers:
      - name: nvidia-device-plugin-ctr
        image: nvcr.io/nvidia/k8s-device-plugin:v0.14.1
        args: ["--fail-on-init-error=false"]
        env:
        - name: FAIL_ON_INIT_ERROR
          value: "false"
        - name: DEVICE_LIST_STRATEGY
          value: "envvar"
        - name: DEVICE_ID_STRATEGY
          value: "uuid"
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
        volumeMounts:
        - name: device-plugin
          mountPath: /var/lib/kubelet/device-plugins
        - name: dev
          mountPath: /dev
      volumes:
      - name: device-plugin
        hostPath:
          path: /var/lib/kubelet/device-plugins
      - name: dev
        hostPath:
          path: /dev
      nodeSelector:
        nvidia.com/gpu.present: "true"
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
---
# ===========================================================
# MIG (Multi-Instance GPU) Configuration
# ===========================================================
apiVersion: nvidia.com/v1
kind: NodePolicy
metadata:
  name: mig-policy
  namespace: gpu-operator
spec:
  mig:
    strategy: "mixed"
    maxInstancesPerNode: 7
---
apiVersion: nvidia.com/v1
kind: ClusterPolicy
metadata:
  name: cluster-policy
  namespace: gpu-operator
spec:
  devicePlugin:
    config:
      name: "default"
      sharing:
        timeSlicing:
          resources:
          - name: nvidia.com/gpu
            replicas: 2
  migManager:
    enabled: true
  nodeStatusExporter:
    enabled: true
  operator:
    defaultRuntime: containerd
  toolkit:
    env:
    - name: WITH_REE
      value: "true"
    - name: REE_ENABLE
      value: "true"
