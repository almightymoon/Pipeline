name: ğŸ”„ Auto Process Repositories

on:
  push:
    paths:
      - 'repos-to-process.yaml'
    branches: [ main ]
  workflow_dispatch:

jobs:
  # ==========================================
  # READ AND PROCESS REPOSITORIES
  # ==========================================
  process-all-repos:
    name: ğŸ”„ Process Repositories
    runs-on: ubuntu-latest
    
    steps:
    - name: ğŸ“¥ Checkout Code
      uses: actions/checkout@v4
    
    - name: ğŸ Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: ğŸ“¦ Install Dependencies
      run: |
        pip install pandas numpy jsonschema pyarrow pyyaml
    
    - name: ğŸ“‹ Read and Process Repositories
      run: |
        python << 'EOF'
        import yaml
        import json
        import subprocess
        import sys
        from pathlib import Path
        
        print("=" * 60)
        print("ğŸ“‹ Reading repos-to-process.yaml")
        print("=" * 60)
        
        # Read configuration
        with open('repos-to-process.yaml') as f:
            config = yaml.safe_load(f)
        
        repos = config.get('repositories', [])
        
        if not repos:
            print("âš ï¸  No repositories found in repos-to-process.yaml")
            print("Add repositories to process and push again!")
            sys.exit(0)
        
        print(f"\nFound {len(repos)} repository(ies) to process:\n")
        
        for idx, repo in enumerate(repos, 1):
            if not repo:  # Skip empty entries
                continue
                
            repo_url = repo.get('repo', '')
            branch = repo.get('branch', 'main')
            repo_type = repo.get('type', 'auto')
            data_format = repo.get('format', 'auto')
            
            print(f"{idx}. {repo_url}")
            print(f"   Branch: {branch}")
            print(f"   Type: {repo_type}")
            print(f"   Format: {data_format}")
            print()
        
        # Save for next steps
        with open('repos-list.json', 'w') as f:
            json.dump(repos, f, indent=2)
        
        print(f"âœ… Configuration loaded - {len(repos)} repositories ready")
        EOF
    
    - name: ğŸ”„ Process Each Repository
      run: |
        python << 'EOF'
        import json
        import subprocess
        import os
        from pathlib import Path
        import shutil
        
        # Load repos list
        with open('repos-list.json') as f:
            repos = json.load(f)
        
        results = []
        
        for idx, repo in enumerate(repos, 1):
            if not repo:
                continue
            
            repo_url = repo.get('repo', '')
            branch = repo.get('branch', 'main')
            repo_type = repo.get('type', 'auto')
            data_format = repo.get('format', 'auto')
            
            print("\n" + "=" * 60)
            print(f"ğŸ”„ Processing Repository {idx}/{len(repos)}")
            print("=" * 60)
            print(f"URL: {repo_url}")
            print(f"Branch: {branch}")
            
            # Clean up previous clone
            if Path('target-repo').exists():
                shutil.rmtree('target-repo')
            
            # Clone repository
            try:
                print(f"\nğŸ“¥ Cloning repository...")
                subprocess.run(['git', 'clone', '--depth=1', '--branch', branch, repo_url, 'target-repo'], 
                             check=True, capture_output=True)
                print(f"âœ… Clone successful")
            except subprocess.CalledProcessError as e:
                print(f"âŒ Clone failed: {e}")
                results.append({"repo": repo_url, "status": "failed", "reason": "clone_failed"})
                continue
            
            # Auto-detect type
            if repo_type == 'auto':
                print(f"\nğŸ” Auto-detecting repository type...")
                os.chdir('target-repo')
                
                # Count files
                csv_files = list(Path('.').rglob('*.csv'))
                json_files = [f for f in Path('.').rglob('*.json') if 'node_modules' not in str(f) and '.git' not in str(f)]
                parquet_files = list(Path('.').rglob('*.parquet'))
                data_files = len(csv_files) + len(json_files) + len(parquet_files)
                
                py_files = list(Path('.').rglob('*.py'))
                js_files = [f for f in Path('.').rglob('*.js') if 'node_modules' not in str(f)]
                code_files = len(py_files) + len(js_files)
                
                print(f"   Data files: {data_files} (CSV: {len(csv_files)}, JSON: {len(json_files)}, Parquet: {len(parquet_files)})")
                print(f"   Code files: {code_files} (Python: {len(py_files)}, JS: {len(js_files)})")
                
                if data_files > code_files and data_files > 0:
                    repo_type = 'dataset'
                    if csv_files:
                        data_format = 'csv'
                    elif parquet_files:
                        data_format = 'parquet'
                    else:
                        data_format = 'json'
                else:
                    repo_type = 'code'
                
                print(f"   âœ… Detected as: {repo_type}")
                os.chdir('..')
            
            # Process based on type
            print(f"\nğŸ¯ Processing as {repo_type.upper()}")
            
            if repo_type == 'dataset':
                print(f"ğŸ“Š Format: {data_format}")
                result = process_dataset(repo_url, data_format)
            else:
                result = process_code(repo_url)
            
            results.append(result)
        
        # Save results
        with open('processing-results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        print("\n" + "=" * 60)
        print("âœ… ALL REPOSITORIES PROCESSED")
        print("=" * 60)
        print(f"\nProcessed: {len(results)} repositories")
        print(f"Success: {sum(1 for r in results if r.get('status') == 'success')}")
        print(f"Failed: {sum(1 for r in results if r.get('status') == 'failed')}")
        
        
        def process_dataset(repo_url, data_format):
            """Process a dataset repository"""
            import pandas as pd
            
            print("ğŸ“Š Processing dataset...")
            
            os.chdir('target-repo')
            output_dir = Path('../processed-output')
            output_dir.mkdir(exist_ok=True)
            
            try:
                if data_format == 'csv':
                    files = list(Path('.').rglob('*.csv'))
                    if files:
                        dfs = [pd.read_csv(f) for f in files[:10]]  # Limit to 10 files
                        combined = pd.concat(dfs, ignore_index=True)
                        combined = combined.dropna().drop_duplicates()
                        
                        combined.to_csv(output_dir / 'processed.csv', index=False)
                        combined.to_json(output_dir / 'processed.json', orient='records', indent=2)
                        
                        print(f"âœ… Processed {len(combined)} rows from {len(files)} files")
                
                elif data_format == 'json':
                    files = [f for f in Path('.').rglob('*.json') if 'node_modules' not in str(f)]
                    all_data = []
                    for file in files[:10]:
                        try:
                            with open(file) as f:
                                data = json.load(f)
                            if isinstance(data, list):
                                all_data.extend(data[:100])  # Limit records per file
                            else:
                                all_data.append(data)
                        except:
                            pass
                    
                    with open(output_dir / 'processed.json', 'w') as f:
                        json.dump(all_data, f, indent=2)
                    
                    print(f"âœ… Processed {len(all_data)} records from {len(files)} files")
                
                os.chdir('..')
                return {"repo": repo_url, "status": "success", "type": "dataset", "records": len(all_data) if data_format == 'json' else len(combined)}
                
            except Exception as e:
                print(f"âŒ Processing failed: {e}")
                os.chdir('..')
                return {"repo": repo_url, "status": "failed", "reason": str(e)}
        
        def process_code(repo_url):
            """Process a code repository"""
            print("ğŸ’» Processing code repository...")
            
            os.chdir('target-repo')
            
            try:
                # Check for Python project
                if Path('requirements.txt').exists():
                    print("  Detected Python project")
                    subprocess.run(['pip', 'install', '-r', 'requirements.txt'], capture_output=True)
                    
                    if Path('tests').exists():
                        result = subprocess.run(['python', '-m', 'pytest', 'tests/', '--tb=short'], 
                                              capture_output=True, text=True)
                        print(f"  Tests: {'âœ… Passed' if result.returncode == 0 else 'âš ï¸  Some failed'}")
                
                # Check for Node.js project
                elif Path('package.json').exists():
                    print("  Detected Node.js project")
                    subprocess.run(['npm', 'install'], capture_output=True)
                
                os.chdir('..')
                print(f"âœ… Code validation completed")
                return {"repo": repo_url, "status": "success", "type": "code"}
                
            except Exception as e:
                print(f"âŒ Processing failed: {e}")
                os.chdir('..')
                return {"repo": repo_url, "status": "failed", "reason": str(e)}
        
        EOF
    
    - name: ğŸ“¤ Upload Processed Data
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: processed-repositories-${{ github.run_number }}
        path: |
          processed-output/
          processing-results.json
        retention-days: 30
    
    - name: ğŸš€ Deploy to Kubernetes
      if: success() && secrets.KUBECONFIG != ''
      run: |
        echo "Deploying to Kubernetes..."
        
        if [ -n "${{ secrets.KUBECONFIG }}" ] && [ -d "processed-output" ]; then
          echo "${{ secrets.KUBECONFIG }}" | base64 -d > $HOME/kubeconfig
          export KUBECONFIG=$HOME/kubeconfig
          
          # Create configmap if we have processed data
          if [ "$(ls -A processed-output 2>/dev/null)" ]; then
            kubectl create configmap processed-repos-$(date +%Y%m%d-%H%M%S) \
              --from-file=processed-output/ \
              --namespace=ml-pipeline \
              --dry-run=client -o yaml | \
            kubectl label -f - app=dataset-processor processed=true --local -o yaml | \
            kubectl apply -f -
            
            echo "âœ… Deployed to Kubernetes"
          else
            echo "âš ï¸  No processed data to deploy"
          fi
        else
          echo "âš ï¸  KUBECONFIG not set or no processed data"
        fi
    
    - name: ğŸ“Š Summary
      if: always()
      run: |
        echo ""
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "âœ… PROCESSING COMPLETE"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo ""
        echo "ğŸ“¦ Check 'Artifacts' section above to download:"
        echo "   - Processed datasets"
        echo "   - Processing results"
        echo "   - Statistics and reports"
        echo ""
        echo "â˜¸ï¸  Kubernetes:"
        echo "   kubectl get configmaps -n ml-pipeline -l app=dataset-processor"
        echo ""
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
