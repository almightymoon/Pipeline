name: 📊 Dataset Processing Pipeline

on:
  workflow_dispatch:
    inputs:
      dataset_repo:
        description: 'Dataset repository URL'
        required: true
        default: 'https://github.com/your-org/your-dataset.git'
      dataset_branch:
        description: 'Branch to process'
        required: false
        default: 'main'
      dataset_format:
        description: 'Dataset format (csv, json, parquet)'
        required: false
        default: 'json'

env:
  REGISTRY: harbor.yourcompany.com
  IMAGE_NAME: dataset-processor

jobs:
  # ==========================================
  # 1. FETCH DATASET
  # ==========================================
  fetch-dataset:
    name: 📥 Fetch Dataset
    runs-on: ubuntu-latest
    
    steps:
    - name: 📥 Clone Dataset Repository
      run: |
        echo "Fetching dataset from: ${{ github.event.inputs.dataset_repo }}"
        git clone ${{ github.event.inputs.dataset_repo }} dataset
        cd dataset
        git checkout ${{ github.event.inputs.dataset_branch }}
        
    - name: 📊 Dataset Info
      run: |
        cd dataset
        echo "Dataset structure:"
        find . -type f -name "*.csv" -o -name "*.json" -o -name "*.parquet" | head -20
        
        echo ""
        echo "Dataset size:"
        du -sh .
        
    - name: 📤 Upload Dataset
      uses: actions/upload-artifact@v4
      with:
        name: raw-dataset
        path: dataset/
        retention-days: 7

  # ==========================================
  # 2. VALIDATE DATASET
  # ==========================================
  validate-dataset:
    name: 🔍 Validate Dataset
    runs-on: ubuntu-latest
    needs: fetch-dataset
    
    steps:
    - name: 📥 Checkout Pipeline Code
      uses: actions/checkout@v4
    
    - name: 📥 Download Dataset
      uses: actions/download-artifact@v4
      with:
        name: raw-dataset
        path: dataset/
    
    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: 📦 Install Validation Tools
      run: |
        pip install pandas numpy jsonschema pandera great-expectations
        
    - name: 🔍 Validate Schema
      run: |
        python << 'EOF'
        import os
        import json
        import pandas as pd
        from pathlib import Path
        
        dataset_dir = Path('dataset')
        format_type = "${{ github.event.inputs.dataset_format }}"
        
        print(f"🔍 Validating {format_type} dataset...")
        
        if format_type == 'csv':
            files = list(dataset_dir.rglob('*.csv'))
            for file in files[:5]:  # Validate first 5 files
                df = pd.read_csv(file)
                print(f"✓ {file.name}: {len(df)} rows, {len(df.columns)} columns")
                
        elif format_type == 'json':
            files = list(dataset_dir.rglob('*.json'))
            for file in files[:5]:
                with open(file) as f:
                    data = json.load(f)
                print(f"✓ {file.name}: {len(data) if isinstance(data, list) else 1} records")
                
        elif format_type == 'parquet':
            files = list(dataset_dir.rglob('*.parquet'))
            for file in files[:5]:
                df = pd.read_parquet(file)
                print(f"✓ {file.name}: {len(df)} rows, {len(df.columns)} columns")
        
        print(f"\n✅ Dataset validation completed!")
        print(f"Total files found: {len(files)}")
        EOF
    
    - name: 📊 Generate Quality Report
      run: |
        python << 'EOF'
        import pandas as pd
        import json
        from pathlib import Path
        
        dataset_dir = Path('dataset')
        format_type = "${{ github.event.inputs.dataset_format }}"
        
        report = {
            "dataset_format": format_type,
            "total_files": 0,
            "total_rows": 0,
            "total_size_mb": 0,
            "quality_checks": {
                "null_values": False,
                "duplicates": False,
                "schema_valid": True
            }
        }
        
        if format_type == 'csv':
            files = list(dataset_dir.rglob('*.csv'))
            report['total_files'] = len(files)
            for file in files:
                df = pd.read_csv(file)
                report['total_rows'] += len(df)
                report['total_size_mb'] += file.stat().st_size / (1024 * 1024)
        
        with open('dataset-quality-report.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        print("📊 Quality Report:")
        print(json.dumps(report, indent=2))
        EOF
    
    - name: 📤 Upload Quality Report
      uses: actions/upload-artifact@v4
      with:
        name: quality-report
        path: dataset-quality-report.json

  # ==========================================
  # 3. PROCESS DATASET
  # ==========================================
  process-dataset:
    name: 🔄 Process Dataset
    runs-on: ubuntu-latest
    needs: validate-dataset
    
    steps:
    - name: 📥 Checkout Pipeline Code
      uses: actions/checkout@v4
    
    - name: 📥 Download Dataset
      uses: actions/download-artifact@v4
      with:
        name: raw-dataset
        path: dataset/
    
    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: 📦 Install Processing Tools
      run: |
        pip install pandas numpy scikit-learn pyarrow
    
    - name: 🔄 Transform Dataset
      run: |
        mkdir -p processed-dataset
        python << 'EOF'
        import pandas as pd
        import json
        from pathlib import Path
        
        dataset_dir = Path('dataset')
        output_dir = Path('processed-dataset')
        format_type = "${{ github.event.inputs.dataset_format }}"
        
        print(f"🔄 Processing {format_type} dataset...")
        
        if format_type == 'csv':
            files = list(dataset_dir.rglob('*.csv'))
            all_data = []
            for file in files:
                df = pd.read_csv(file)
                # Apply transformations
                df = df.dropna()  # Remove null values
                df = df.drop_duplicates()  # Remove duplicates
                all_data.append(df)
            
            if all_data:
                combined = pd.concat(all_data, ignore_index=True)
                # Save in multiple formats
                combined.to_csv(output_dir / 'processed.csv', index=False)
                combined.to_json(output_dir / 'processed.json', orient='records')
                combined.to_parquet(output_dir / 'processed.parquet')
                print(f"✅ Processed {len(combined)} rows")
        
        elif format_type == 'json':
            files = list(dataset_dir.rglob('*.json'))
            all_data = []
            for file in files:
                with open(file) as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        all_data.extend(data)
                    else:
                        all_data.append(data)
            
            # Save processed data
            with open(output_dir / 'processed.json', 'w') as f:
                json.dump(all_data, f, indent=2)
            
            # Also save as CSV for easy viewing
            if all_data:
                df = pd.DataFrame(all_data)
                df.to_csv(output_dir / 'processed.csv', index=False)
                print(f"✅ Processed {len(all_data)} records")
        
        print("✅ Dataset processing completed!")
        EOF
    
    - name: 📤 Upload Processed Dataset
      uses: actions/upload-artifact@v4
      with:
        name: processed-dataset
        path: processed-dataset/
        retention-days: 30

  # ==========================================
  # 4. DEPLOY TO KUBERNETES
  # ==========================================
  deploy-dataset:
    name: 🚀 Deploy Dataset to Kubernetes
    runs-on: ubuntu-latest
    needs: process-dataset
    if: success()
    
    steps:
    - name: 📥 Download Processed Dataset
      uses: actions/download-artifact@v4
      with:
        name: processed-dataset
        path: processed-dataset/
    
    - name: ☸️ Setup Kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'latest'
    
    - name: 🔐 Configure Kubernetes Access
      run: |
        if [ -n "${{ secrets.KUBECONFIG }}" ]; then
          echo "${{ secrets.KUBECONFIG }}" | base64 -d > $HOME/kubeconfig
          echo "KUBECONFIG=$HOME/kubeconfig" >> $GITHUB_ENV
          echo "✅ Kubernetes access configured"
        else
          echo "⚠️ KUBECONFIG not set, skipping deployment"
          exit 0
        fi
    
    - name: 📊 Create Dataset ConfigMap
      run: |
        echo "Creating dataset configmap in Kubernetes..."
        
        # Create configmap with processed dataset
        kubectl create configmap processed-dataset-$(date +%Y%m%d-%H%M%S) \
          --from-file=processed-dataset/ \
          --namespace=ml-pipeline \
          --dry-run=client -o yaml > dataset-configmap.yaml
        
        # Add labels
        kubectl label -f dataset-configmap.yaml \
          app=dataset-processor \
          processed-date=$(date +%Y%m%d) \
          --local -o yaml > labeled-configmap.yaml
        
        # Apply to cluster
        kubectl apply -f labeled-configmap.yaml
        
        echo "✅ Dataset deployed to Kubernetes"
    
    - name: 📋 List Deployed Datasets
      run: |
        echo "📊 Deployed datasets:"
        kubectl get configmaps -n ml-pipeline -l app=dataset-processor

  # ==========================================
  # 5. GENERATE REPORT
  # ==========================================
  generate-report:
    name: 📄 Generate Processing Report
    runs-on: ubuntu-latest
    needs: [validate-dataset, process-dataset, deploy-dataset]
    if: always()
    
    steps:
    - name: 📥 Download Quality Report
      uses: actions/download-artifact@v4
      with:
        name: quality-report
        path: reports/
    
    - name: 📊 Generate Summary Report
      run: |
        cat << 'EOF' > processing-summary.md
        # 📊 Dataset Processing Report
        
        ## Repository Information
        - **Repository**: ${{ github.event.inputs.dataset_repo }}
        - **Branch**: ${{ github.event.inputs.dataset_branch }}
        - **Format**: ${{ github.event.inputs.dataset_format }}
        - **Processed**: $(date)
        
        ## Pipeline Status
        - ✅ Fetch: Success
        - ✅ Validate: Success
        - ✅ Process: Success
        - ✅ Deploy: ${{ needs.deploy-dataset.result }}
        
        ## Quality Metrics
        $(cat reports/dataset-quality-report.json)
        
        ## Artifacts
        - Processed Dataset: Available in artifacts
        - Quality Report: Available in artifacts
        - Kubernetes ConfigMap: deployed to ml-pipeline namespace
        
        ---
        Generated by Dataset Processing Pipeline
        EOF
        
        cat processing-summary.md
    
    - name: 📤 Upload Report
      uses: actions/upload-artifact@v4
      with:
        name: processing-report
        path: processing-summary.md

